<article>
 <preambule> L18-1504.pdf </preambule>
 <titre>A New Annotated Portuguese/Spanish Corpus for the Multi-Sentence Compression Task</titre>
 <auteurs>
 <auteur>Elvys Linhares Pontes : elvys.linhares-pontes@univ-avignon.fr</auteur>
 <auteur>Juan-Manuel Torres-Moreno : juan-manuel.torres@univ-avignon.fr</auteur>
 <auteur>Stéphane Huet : stephane.huet@univ-avignon.fr</auteur>
 <auteur>Andréa Carneiro Linhares : andrea.linhares@ufc.br</auteur>
 </auteurs>
 <affiliations>CERI/LIA, Université d’Avignon et des Pays de Vaucluse, Avignon, France , ́Ecole Polytechnique de Montréal, Montréal, Canada, Universidade Federal do Ceara, Sobral-CE, Brasil</affiliations>
 <abstract> Multi-sentence compression aims to generate a short and informative compression from several source sentences that deal with the sametopic. In this work, we present a new corpus for the Multi-Sentence Compression (MSC) task in Portuguese and Spanish. We alsoprovide on this corpus a comparison of two state-of-the-art MSC systems.
 </abstract>
 <introduction>Among the various applications of Natural Language Pro-cessing, Automatic Text Summarization (ATS) aims atsummarizing one or more texts automatically. Summariza-tion systems identify relevant data and create a summaryfrom key information. The (Multi-)Sentence Compressiontask can be seen as a subproblem of ATS with the objectiveto generate a shorter, informative and correct sentence fromsource sentence(s).In many cases, state-of-the-art NLP systems are evaluatedwith experiments restrained to the English language, in partbecause there are a lot of available English resources formost NLP tasks. As regards Multi-Sentence Compression(MSC), the available resources are unfortunately limited; toour knowledge, only one dataset is freely available and it isconfined to the French language (Boudin and Morin, 2013).In this work, we present a new annotated corpus in the Por-tuguese and Spanish languages for the MSC task. Usingthis corpus, we evaluate two state-of-the-art systems andshow that the use of several languages leads to more miti-gated results on the superiority of one system than the useof the French corpus alone.The remainder of this paper is organized as follows. In Sec-tion 2, we characterize MSC with respect to related tasksfrom the perspective of the available corpora. Section 3describes the creation and the features of our corpus. InSection 4 we analyze the results achieved by state-of-the-art methods using our dataset. Finally, conclusions are setout in Section 5.</introduction>
 <corps></corps>
 <conclusion> Multi-Sentence Compression aims to generate a short infor-mative text summary from several sentences with relatedand redundant information. This task can be used in thedomain of multi-document summarization or question an-swering to provide more informative and concise texts.In this paper, we presented a new annotated corpus intwo languages that extends the French data made availablein (Boudin and Morin, 2013). We also compared two state-of-the art systems on this new dataset. We hope this cor-pus will help the NLP community to develop and validatemulti-language methods for multi-sentence compression.In order to extend the multi-language resources to more di-verse languages, we plan to create a similar MSC datasetfor Arabic. We also want to use our corpus to test othercompetitive MSC systems, such as the one based on integerlinear programming we introduced in (Linhares Pontes etal., 2016)</conclusion>
 <discussion></discussion>
 <biblio>Barzilay, R. and McKeown, K. R. (2005). Sentence fusionfor multidocument news summarization.ComputationalLinguistics, 31(3):297–328, September.Boudin, F. and Morin, E. (2013). Keyphrase extractionfor N-best reranking in multi-sentence compression. InNAACL, pages 298–305.Clarke, J. and Lapata, M. (2008). Global inference forsentence compression: An integer linear programmingapproach.Journal of Artificial Intelligence Research(JAIR, 31:399–429.Cohn, T. and Lapata, M. (2008). Sentence compressionbeyond word deletion. InCOLING, pages 137–144.Filippova, K. and Altun, Y. (2013). Overcoming the lack ofparallel data in sentence compression. InEMNLP, pages1481–1491.Filippova, K., Alfonseca, E., Colmenares, C. A., Kaiser,L., and Vinyals, O. (2015). Sentence compression bydeletion with LSTMs. InEMNLP, pages 360–368.Filippova, K. (2010). Multi-sentence compression: Find-ing shortest paths in word graphs. InCOLING, pages322–330.Ganitkevitch, J., Callison-Burch, C., Napoles, C., andDurme, B. V. (2011). Learning sentential paraphrasesfrom bilingual parallel corpora for text-to-text genera-tion. InEMNLP, pages 1168–1179.Ive, J. and Yvon, F. (2016). Parallel sentence compression.InCOLING, Technical Papers, page 1503–1513.Knight, K. and Marcu, D. (2002). Summarization beyondsentence extraction: A probabilistic approach to sentencecompression.Artificial Intelligence, 139(1):91–107.Lin, C.-Y. (2004). ROUGE: A Package for AutomaticEvaluation of Summaries. InWorkshop Text Summariza-tion Branches Out (ACL’04), pages 74–81.Linhares Pontes, E., Gouveia da Silva, T., Linhares,A. C., Torres-Moreno, J.-M., and Huet, S. (2016).M ́etodos de otimizac ̧ ̃ao combinat ́oria aplicados ao prob-lema de compress ̃ao multifrases. InAnais do XLVIIISimp ́osio Brasileiro de Pesquisa Operacional (SBPO),pages 2278–2289.Luong, A. V., Tran, N. T., Ung, V. G., and Nghiem, M. Q.(2015). Word graph-based multi-sentence compression:Re-ranking candidates using frequent words. InSev-enth International Conference on Knowledge and Sys-tems Engineering (KSE), pages 55–60.McKeown, K., Rosenthal, S., Thadani, K., and Moore, C.(2010). Time-efficient creation of an accurate sentencefusion corpus. InHLT-NAACL, pages 317–320.Rush, A. M., Chopra, S., and Weston, J. (2015). A neuralattention model for abstractive sentence summarization.InEMNLP, pages 379–389.Thadani, K. and McKeown, K. (2013). Supervised sen-tence fusion with single-stage inference. InSixth Inter-national Joint Conference on Natural Language Process-ing, IJCNLP, pages 1410–1418.Toutanova, K., Brockett, C., Tran, K. M., and Amershi,S. (2016). A dataset and evaluation metrics for abstrac-tive compression of sentences and short paragraphs. InEMNLP, pages 340–350. 8. Language Resource ReferencesBoudin,Florian and Morin,Emmanuel.(2013).Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression. NAACL (2013). Available onhttps://github.com/boudinfl/lina-msc.Graff, David and Cieri, Christopher and Kong, Junbo andChen, Ke and Maeda, Kazuaki. (2011).English Gi-gaword. Linguistic Data Consortium, 5th, ISLRN 911-942-430-413-0.</biblio>
</article>
