<article>
 <preambule>b0e5c43edf116ce2909ae009cc27a1546f09.pdf </preambule>
 <titre>Inclusive yet Selective: Supervised Distributional Hypernymy Detection</titre>
 <auteurs>
 <auteur>Stephen Roller : roller@cs.utexas.edu</auteur>
 <auteur>Katrin Erk : katrin.erk@mail.utexas.edu</auteur>
 <auteur>Gemma Boleda : gemma.boleda@upf.edu</auteur>
 </auteurs>
 <affiliations>Department of Computer Science, Department of Linguistics, The University of Texas at Austin</affiliations>
 <abstract> We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur ina superset of contexts in which their hyponyms are found. We find that this hypothesis onlyholds when it is applied to relevant dimensions. We propose a robust supervised approach thatachieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selectingthe dimensions that are relevant for distributional inclusion </abstract>
 <introduction>One of the main criticisms of distributional models has been that they fail to distinguish between semanticrelations: Typical nearest neighbors ofdogare words likecat,animal,puppy,tail, orowner, all obviouslyrelated todog, but through very different types of semantic relations. On these grounds, Murphy (2002)argues that distributional models cannot be a valid model of conceptual representation. Distinguishingsemantic relations are also crucial for drawing inferences from distributional data, as different semanticrelations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such asRecognizing Textual Entailment or RTE (Geffet and Dagan, 2004).For these reasons, research has in recent years started to attempt the detection of specific semanticrelationships, and current results suggest that distributional models can, in fact, distinguish betweensemantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenciand Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTEand other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relationbetween a superordinate term in a taxonomy (e.g.animal) and a subordinate term (e.g.dog).Distributional approaches to date for detecting hypernymy, and the related but broader relation oflexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been basedon the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffetand Dagan, 2009), which states that more specific terms appear in a subset of the distributional contextsin which more general terms appear. So,animalcan occur in all the contexts in whichdogcan occur,plus some contexts in whichdogcannot – for instance,rightscan be a typical cooccurrence foranimal(e.g. “animal rights”), but not so much fordog(e.g. #“dog rights”).This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. Weshow that the current best unsupervised approach is brittle in that their performance depends on the spacethey are applied to. This raises the question of whether the Distributional Inclusion Hypothesis is correct,and if so, under what circumstances it holds. We use a simple supervised approach to relation detectionthat has good performance (accuracy .84 on BLESS, .85 on the lexical entailment dataset of Baroni etal. (2012)) and works well across different spaces.1Furthermore, we show that it can be interpretedas selecting dimensions for which the Distributional Inclusion Hypothesis does hold. So, our answer isto propose theSelective Distributional Inclusion Hypothesis: The Distributional Inclusion Hypothesisholds, but only for relevant dimensions.</introduction>
 <corps></corps>
 <conclusion> In this paper, we have tested the Distributional Inclusion Hypothesis, the basis for distributional ap-proaches to hypernymy. We have found that the hypothesis only works if inclusion is selectively appliedto a set of relevant dimensions. We have tested two simple supervised approaches to distributional hypernymy detection and havefound that they show good performance, and are robust to changes in the underlying space. Our bestclassifier achieves .84 accuracy on BLESSand .85 on the ENTAILMENTdataset of Baroni et al. (2012). Ituses features that encode dimension-wise difference between vectors. This classifier can be interpretedas selecting the dimensions necessary for the Distributional Inclusion Hypothesis to work, thus as aneffective way to implementselectivedistributional inclusion.The next natural step is to use the supervised features to guide development of an unsupervised mea-sure for hypernymy detection: Now that we have examples, we hope to propose a method which selectsrelevant features automatically. We also would like to explore detection of other relationships, suchas meronymy. Finally, we would like to perform an extrinsic evaluation of our hypernymy detectionapproach in an actual RTE system.</conclusion>
 <discussion></discussion>
 <biblio> Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. InProceedingsof the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1–10, Edinburgh,UK, July. Association for Computational Linguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: Acollection of very large linguistically processed web-crawled corpora.Language Resources and Evaluation,43(3):209–226.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the wordlevel in distributional semantics. InProceedings of the 13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 23–32, Avignon, France, April. Association for ComputationalLinguistics.Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. InProceedings of the 37thAnnual Meeting of the Association for Computational Linguistics, pages 57–64, College Park, Maryland, USA,June. Association for Computational Linguistics.Paul Buitelaar, Philipp Cimiano, and Bernardo Magnini. 2005.Ontology Learning from Text: Methods, Evaluationand Applications. Frontiers in Artificial Intelligence and Applications Series. IOS Press, Amsterdam.Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations.InProceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 33–40.Philipp Cimiano, Aleksander Pivk, Lars Schmidt-Thieme, and Steffen Staab. 2005. Learning taxonomic relationsfrom heterogeneous sources of evidence.Ontology Learning from Text: Methods, evaluation and applications.Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. InProceedings of theWorkshop on Geometrical Models of Natural Language Semantics, pages 112–119, Athens, Greece, March.Association for Computational Linguistics.Maayan Geffet and Ido Dagan. 2004. Feature vector quality and distributional similarity. InProceedings of the20th International Conference on Computational Linguistics, page 247. Association for Computational Linguis-tics.Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automaticdiscovery of part-whole relations. InProceedings of the 2003 Conference of the North American Chapter of theAssociation for Computational Linguistics on Human Language Technology-Volume 1, pages 1–8. Associationfor Computational Linguistics.10http://www.tacc.utexas.edu Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations.Com-putational Linguistics, 32(1):83–135.Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. InProceedings of the 14thConference on Computational Linguistics, pages 539–545, Stroudsburg, PA, USA. Association for Computa-tional Linguistics.Aur ́elie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectors. InProceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: ShortPapers), pages 440–445, Sofia, Bulgaria, August. Association for Computational Linguistics.Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-geffet. 2010. Directional distributionalsimilarity for lexical inference.Natural Language Engineering, 16:359–389, 10.Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In*SEM2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of themain conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 75–79, Montr ́eal, Canada, 7-8 June. Association for ComputationalLinguistics.Alessandro Lenci. 2008. Distributional approaches in linguistic and cognitive research.Italian Journal of Lin-guistics, 20(1):1–31.Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionallysimilar words. InProceedings of the 18th international Joint Conference on Artificial intelligence, pages 1492–1493.Dekang Lin. 1998. An information-theoretic definition of similarity. InProceedings of the 15th InternationalConference on Machine Learning, volume 98, pages 296–304.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-resentations. InProceedings of the 2013 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June. Associa-tion for Computational Linguistics.Gregory L. Murphy. 2002.The Big Book of Concepts. MIT Press, Boston, MA.Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvestingsemantic relations. InProceedings of the 21st International Conference on Computational Linguistics and the44th annual meeting of the Association for Computational Linguistics.Fabian Pedregosa, Ga ̈el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertran Thirion, Olivier Grisel, MathieuBlondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Courna-peau, Matthieu Brucher, MMatthieu Perrot, and ́Edouard Duchesnay. 2011. Scikit-learn: Machine learning inPython.Journal of Machine Learning Research, 12:2825–2830.Enrico Santus. 2013. SLQS: An entropy measure. Master’s thesis, University of Pisa.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym dis-covery. In Lawrence K. Saul, Yair Weiss, and L ́eon Bottou, editors,Advances in Neural Information ProcessingSystems 17, pages 1297–1304, Cambridge, MA. MIT Press.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence.InProceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meetingof the Association for Computational Linguistics, ACL-44, pages 801–808, Stroudsburg, PA, USA. Associationfor Computational Linguistics.Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics.Journalof Artificial Intelligence Research, 37:141–188.Peter D. Turney. 2006. Similarity of semantic relations.Computational Linguistics, 32(3):379–416.Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and MarkSteedman, editors,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,pages 81–88.Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.InProceedings of the 20th International Conference on Computational Linguistics, pages 1015–1021, Geneva,Switzerland, Aug 23–Aug 27. Association for Computational Linguistics, COLING.Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment.InProceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 107–114,Ann Arbor, Michigan, June. Association for Computational Linguistics.Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping distributional feature vector quality.Computa-tional linguistics, 35(3):435–461</biblio>
</article>
