<article>
      <preambule> BLESS.pdf </preambule>
      <titre> How we BLESSed distributional semantic evaluation </titre>
      <auteurs>
      <auteur>Marco Baroni  : marco.baroni@unitn.it</auteur>
      <auteur> Alessandro Lenci : alessandro.lenci@ling.unipi.it</auteur>
      </auteurs>
      <affiliations>University of Trento Trento, Italy University of Pisa Pisa, Italy  </affiliations>
      <abstract> We introduce BLESS, a data set specifically We introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models. BLESS contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. It is thus possible to assess the ability of a model to detect truly related word pairs, as well as to perform in-depth analyses of the types of semantic relations that a model favors. We discuss the motivations for BLESS, describe its construction and structure, and present examples of its usage in the evaluation of distributional semantic models.  </abstract>
      <introduction>  In NLP, it is customary to distinguish between intrinsic evaluations, testing a system in itself, and extrinsic evaluations, measuring its performance in some task or application (Sparck Jones and Galliers, ). For instance, the intrinsic evaluation of a dependency parser will measure its accuracy in identifying specific syntactic relations, while its extrinsic evaluation will focus on the impact of the parser on tasks such as question answering or machine translation. Current approaches to the evaluation of Distributional Semantic Models (DSMs, also known as semantic spaces, vector-space models, etc.; see Turney and Pantel (2010) for a survey) are taskoriented. Model performance is evaluated in “semantic tasks”, such as detecting synonyms, recognizing analogies, modeling verb selectional preferences, ranking paraphrases, etc. Measuring the performance of DSMs on such tasks represents an indirect test of their ability to capture lexical meaning. The task-oriented benchmarks adopted in distributional semantics have not specifically been designed to evaluate DSMs. For instance, the widely used TOEFL synonym detection task was designed to test the learners’ proficiency in English as a second language, and not to investigate the structure of their semantic representations (cf. Section 2)
To gain a real insight into the abilities of DSMs to address lexical semantics, existing benchmarks must be complemented with a more intrinsically oriented approach, to perform direct tests on the specific aspects of lexical knowledge captured by the models
In order to achieve this goal, three conditions must be met: (i) to single out the particular aspects of meaning that we want to focus on in the evaluation of DSMs; (ii) to design a data set that is able to explicitly and reliably encode the target semantic information; (iii) to specify the evaluation criteria of the system performance on the data set, in order to get an estimate of the intrinsic ability of DSMs to cope with the selected semantic aspects. In this paper, we address these three conditions by presenting BLESS (Baroni and Lenci Evaluation of Semantic Spaces), a new data set specifically geared towards the intrinsic evaluation of DSMs, downloadable from: http://clic.cimec.unitn.it/distsem
 </introduction>
      <corps>  2 Distributional semantics benchmarks There are several benchmarks that have been widely adopted for the evaluation of DSMs, all of them capturing interesting challenges a DSM should meet
We briefly review here some commonly used and representative benchmarks, and discuss why we felt  the need to add BLESS to the set. We notice at the  </corps>
      <conclusion>  We introduced BLESS, the first data set specifically designed for the intrinsic evaluation of DSMs. The data set contains tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. Thus, BLESS can be used to evaluate both the ability of DSMs to discriminate truly related word pairs, and to perform in-depth analyses of the types of semantic relata that different models tend to favor among the nearest neighbors of a target concept. Even a simple comparison of the performance of a few DSMs on BLESS - like the one we have shown here - is able to highlight interesting differences in the semantic spaces produced by the various models. The success of BLESS will obviously depend on whether it will become a reference model for the evaluation of DSMs, something that can not be foreseen a priori. Whatever its destiny, we believe that the BLESS approach can boost and innovate evaluation in distributional semantics, as a key condition to get at a deeper understanding of its potentialities as a viable model for meaning
  </conclusion>
      <discussion>  space for BLESS, and not to detract from the importance and usefulness of other data sets. We further remark that we focus on data sets that, like BLESS, are monolingual English and, while task-oriented, not aimed at a specific application setting (such as machine translation or ontology population)
Probably the most commonly used benchmark in distributional semantics is the TOEFL synonym detection task introduced to computational linguistics by Landauer and Dumais (1997). It consists of  multiple-choice questions, each made of a target word (a noun, verb, adjective or adverb) and 4 response words, 1 of them a synonym of the target
For example, given the target levied, the matched words are imposed, believed, requested, correlated, the first one being the correct choice. The task for a system is then to pick the true synonym among the responses. The TOEFL task focuses on a single semantic relation, namely synonymy. Synonymy is actually not a common semantic relation and one of the hardest to define, to the point that many lexical semanticists have concluded that true synonymy does not exist (Cruse, 1986). Just looking at a few examples of synonym pairs from the TOEFL set will illustrate the problem: discrepancy/difference, prolific/productive, percentage/proportion, to market/to sell, color/hue. Moreover, the criteria adopted to choose the distractors (probably motivated by the language proficiency testing purposes of TOEFL) are not known. By looking at the set, it is hard to discern a coherent pattern. In certain cases, the distractors are semantically close to the target word (volume, sample and profit for percentage), whereas in other cases they are not (home, trail, and song for annals). It it thus not clear whether we are asking the models to distinguish a semantically related word (the synonym) from random elements, or a more tightly related word (the synonym, again) from other related words. The TOEFL task, finally, is based on a discrete choice (either you get the right word, or you don’t), with the result that evaluation is “quantized”, leading to large accuracy gains for small actual differences (one model that guesses one more synonym right than another gets 1.25% more points in percentage accuracy)
The WordSim 353 data set (Finkelstein et al., ) is a widely used example of semantic similarity rating set (see also Rubenstein and Goodenough (1965) and Miller and Charles (1991)). Subjects were asked to rate a set of 353 word pairs on a “similarity” scale and average ratings for each pair were computed. Models are then evaluated in terms of correlation of their similarity scores with average ratings across pairs. From the point of view of assessing the performance of a DSM, the WordSim (and related) similarity ratings are a mixed bag, in two senses. First, the data set contains a variety of different semantic relations. In a recent semantic annotation of the WordSim performed by Agirre et al. (2009) we find that, among the 174 pairs with above-median score (and thus presumably related), there is 1 identical pair, 17 synonym pairs, 28 hyper-/hyponym pairs, 30 coordinate pairs,  holo-/meronym pairs and 92 (more than half) pairs that are “topically related, but none of the above”
Second, the scores are a mixture of intuitions about which of these relations are more semantically tight and intuitions about more or less connected pairs within each of the relations. For example, among the top-rated scores we find synonyms such as journey/voyage and coordinate concepts (king/queen)
If we look at the relations characterizing pairs around the median rating, we find both less “perfect” synonyms (monk/brother, that are synonymous only under an unusual sense of brother) and less close coordinates (skin/eye), as well as pairs instantiating other, less taxonomically tight relations, such as many syntagmatically connected items (family/planning, disaster/area, bread/butter). Apparently, a single scale is merging intuitions about semantic similarity of specific pairs and semantic similarity of different relations
A perhaps more principled way to evaluate DSMs that has recently gained some popularity is the concept categorization task, where a DSM has to cluster a set of nouns expressing basic-level concepts into gold standard categories. A particularly carefully constructed example is the Almuhareb-Poesio (AP) set of 402 concepts introduced in Almuhareb (2006). Concept categorization sets also include the Battig (Baroni et al., 2010) and ESSLLI 2008 (Baroni et al., 2008) lists. The AP concepts must be clustered into 21 classes, each represented by between 13 and 21 nouns. Examples include the ve hicle class (helicopter, motorcycle...), the motivation class (ethics, incitement, ...), and the social unit class (platoon, branch). The concepts are balanced in terms of frequency and ambiguity, so that, e.g., the tree class contains a common concept such as pine but also the casuarina tree, as well as the samba tree, that is not only an ambiguous term, but one where the non-arboreal sense dominates
Concept categorization data sets, while interesting to simulate one of the basic aspects of human cognition, are limited to one kind of semantic relation (discovering coordinates). More importantly, the quality of the results will depend not only on the underlying DSMs, but also on the clustering algorithm being used (and on how this interacts with the overall structure of the DSM), thus making it hard to interpret the performance of DSMs. The forced “hard” category choice is also problematic, and exaggerates performance differences between models especially in the presence of ambiguous terms (a model that puts samba in the occasion class with dance and ball might be penalized as much as a model that puts it in the monetary currency class)
A more general issue with all benchmarks is that tasks are based on comparing a single quality score for each considered model (accuracy for TOEFL, correlation for WordSim, a clustering quality measure for AP, etc.). This gives little insight into how and why the models differ. Moreover, there is no well-established statistical procedure to assess significance of differences for most commonly used measures. Finally, either because the data sets were not originally intended as standard benchmarks, or even on purpose, they all are likely to cause coverage problems even for DSMs trained on very large corpora. Think of the presence of extremely rare nouns like casuarina in AP, of proper nouns in WordSim (it is not clear to us that DSMs are adequate semantic models for referring expressions – at the very least they should not be mixed up lightly with common nouns), or multi-word expressions in other data sets
 How we intend to BLESS distributional semantic evaluation DSMs measure the distributional similarity between words, under the assumption that proximity in distributional space models semantic relatedness, including, as a special case, semantic similarity (Budanitsky and Hirst, 2006). However, semantically related words in turn differ for the type of relation holding between them: e.g., dog is strongly related to both animal and tail, but with different types of relations. Therefore, evaluating the intrinsic ability of DSMs to represent the semantic space of a word entails both (i) determining to what extent words close in semantic space are actually semantically related, and (ii) analyzing, among related words, which type of semantic relation they tend to instantiate. Two models can be equally very good in identifying semantically related words, while greatly differing for the type of related pairs they favor
The BLESS data set complies with both these constraints. The set is populated with tuples expressing a relation between a target concept (henceforth referred to as concept) and a relatum concept (henceforth referred to as relatum). For instance, in the BLESS tuple coyote-hyper-animal, the concept coyote is linked to the relatum animal via the hypernymy relation (the relatum is a hypernym of the concept). BLESS focuses on a coherent set of basiclevel nominal concrete concepts and a small but explicit set of semantic relations, each instantiated by multiple relata. Depending on the type of relation, relata can be nouns, verbs or adjectives. Moreover, BLESS also contains, for each concept, a number of random “relatum” words that are not semantically related to the concept. Thus, it also allows to evaluate a model in terms of its ability to harvest related words given a concept (by comparing true and random relata), and to identify specific types of relata, both in terms of semantic relation and part of speech
A data set intending to represent a gold standard for evaluation should include tests items that are as little controversial as possible. The choice of restricting BLESS to concrete concepts is motivated by the fact that they are by far the most studied ones, and there is better agreement about the relations that characterize them (Murphy, 2002; Rogers and McClelland, 2004)
As for the types of relation to include, we are faced with a dilemma. On the one hand, there is wide evidence that taxonomic relations, the best understood type, only represent a tiny portion of the rich spectrum covered by semantic relatedness. On the other hand, most of these wider semantic rela tions are also highly controversial, and may easily lead to questionable classifications. For instance, concepts are related to events, but often it is not clear how to distinguish the events expressing a typical function of nominal concepts (e.g., car and transport), from those events that are also strongly related to them but without representing their typical function sensu stricto (e.g., car and fix). As will be shown in Section 4, the BLESS data set tries to overcome this dilemma by attempting a difficult compromise: Semantic relations are not limited to taxonomic types and also include attributes and events strongly related to a concept, but in these cases we have resorted to underspecification, rather than committing ourselves to questionable granular relations
BLESS strives to capture those differences and similarities among DSMs that do not depend on  </discussion>
      <biblio>  Herv Abdi and Lynne Williams. 2010. Newman-Keuls and Tukey test. In N.J. Salkind, D.M. Dougherty, and B. Frey, editors, Encyclopedia of Research Design
Sage, Thousand Oaks, CA
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasça, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of HLT-NAACL, pages 19–27, Boulder, CO
Abdulrahman Almuhareb. 2006. Attributes in Lexical Acquisition. Phd thesis, University of Essex
Marco Baroni and Alessandro Lenci. 2010. Distributional Memory: A general framework for corpus-based semantics. Computational Linguistics, (4):673–721
Marco Baroni, Stefan Evert, and Alessandro Lenci, editors. 2008. Bridging the Gap between Semantic Theory and Computational Simulations: Proceedings of the ESSLLI Workshop on Distributional Lexical Semantic. FOLLI, Hamburg
Marco Baroni, Eduard Barbu, Brian Murphy, and Massimo Poesio. 2010. Strudel: A distributional semantic model based on properties and types. Cognitive Science, 34(2):222–254
Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32:13–47
D. A. Cruse. 1986. Lexical Semantics. Cambridge University Press, Cambridge
Stefan Evert. 2005. The Statistics of Word Cooccurrences. Dissertation, Stuttgart University
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, (1):116–131
Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240
Hugo Liu and Push Singh. 2004. ConceptNet: A practical commonsense reasoning toolkit. BT Technology Journal, pages 211–226
Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, 28:203–208
Ken McRae, George Cree, Mark Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547–559
George Miller and Walter Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28
Gregory Murphy. 2002. The Big Book of Concepts. MIT Press, Cambridge, MA
Timothy Rogers and James McClelland. 2004. Semantic Cognition: A Parallel Distributed Processing Approach. MIT Press, Cambridge, MA
Herbert Rubenstein and John Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633
Karen Sparck Jones and Julia R. Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. Springer Verlag, Berlin
Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188
Peter Turney. 2006. Similarity of semantic relations
Computational Linguistics, 32(3):379–416
Morton E. Winston, Roger Chaffin, and Douglas Herrmann. 1987. A taxonomy of part-whole relations
Cognitive Science, 11:417–444
  </biblio>
</article>
